{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c316c97bcc2beeba",
   "metadata": {},
   "source": [
    "# Total Variation Distance\n",
    "\n",
    "## Definition\n",
    "Let $P$ and $Q$ be probability measures with a discrete sample space $E$ and probability mass functions $f$ and $g$. Then, the total variation distance between $P$ and $Q$:$$\\text {TV}(\\mathbf{P}, \\mathbf{Q}) = {\\max _{A \\subset E}}| \\mathbf{P}(A) - \\mathbf{Q}(A) |$$\n",
    "can be computed as$$\\text {TV}(\\mathbf{P}, \\mathbf{Q}) = \\frac{1}{2} \\, \\sum _{x \\in E} |f(x) - g(x)|$$\n",
    "Let $P$ and $Q$ be probability distributions on a continuous sample space $E$ with probability density functions $f$ and $g$. Then, the total variation distance between $P$ and $Q$ $$\\text {TV}(\\mathbf{P}, \\mathbf{Q}) = {\\max _{A \\subset E}}| \\mathbf{P}(A) - \\mathbf{Q}(A) |$$\n",
    "can be computed as$$\\text {TV}(\\mathbf{P}, \\mathbf{Q}) = \\frac{1}{2} \\, \\int  _{x \\in E} |f(x) - g(x)|~ \\text {d}x$$\n",
    "\n",
    "## Properties\n",
    "$TV(\\mathbf{P}, \\mathbf{Q}) = TV(\\mathbf{Q}, \\mathbf{P})$ (symmetric)  \n",
    "$TV(\\mathbf{P}, \\mathbf{Q}) \\geq 0$ (nonnegative)  \n",
    "$TV(\\mathbf{P}, \\mathbf{Q}) = 0 \\iff \\mathbf{P}= \\mathbf{Q}$(definite)  \n",
    "$TV(\\mathbf{P}, \\mathbf{V}) \\leq TV(\\mathbf{P}, \\mathbf{Q}) + TV(\\mathbf{Q}, \\mathbf{V})$(triangle inequality)  \n",
    "These imply that the total variation is a distance between probability distributions.  \n",
    "The smallest number $M$ such that $TV(P,Q)≤M$ for any probability measures $P,Q$ is 1"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Kullback-Leibler (KL) Divergence\n",
    "\n",
    "## Definition\n",
    "Let $P$ and $Q$ be discrete probability distributions with pmfs $p$ and $q$ respectively. Let's also assume $P$ and $Q$ have a common sample space $E$. Then the KL divergence (also known as relative entropy ) between $P$ and $Q$ is defined by$$\\text {KL}(\\mathbf{P}, \\mathbf{Q}) = \\sum _{x \\in E} p(x) \\ln \\left( \\frac{p(x)}{q(x)} \\right)$$where the sum is only over the support of $P$.  \n",
    "Analogously, if $P$ and $Q$ are continuous probability distributions with pdfs $p$ and $q$ on a common sample space $E$, then$$\\text {KL}(\\mathbf{P}, \\mathbf{Q}) = {{\\int }} _{x \\in E} p(x) \\ln \\left( \\frac{p(x)}{q(x)} \\right) dx$$where the integral is again only over the support of $P$.\n",
    "\n",
    "## Properties\n",
    "$KL(\\mathbf{P}, \\mathbf{Q}) \\neq KL(\\mathbf{Q}, \\mathbf{P})$ in general(Asymmetric)  \n",
    "$KL(\\mathbf{P}, \\mathbf{Q}) \\geq 0$ (nonnegative)  \n",
    "$KL(\\mathbf{P}, \\mathbf{Q}) = 0$ only if $P$ and $Q$ are the same distribution (definite)  \n",
    "$KL(\\mathbf{P}, \\mathbf{V}) \\nleq KL(\\mathbf{P}, \\mathbf{Q}) + KL(\\mathbf{Q}, \\mathbf{V})$ in general  \n",
    "\n",
    "Not a distance.  \n",
    "This is called a divergence.  \n",
    "Asymmetry is the key to our ability to estimate it.  \n",
    "$θ^∗$ Is the unique minimizer of $θ \\mapsto KL(P_{θ^∗},P_θ)$\n",
    "\n",
    "## Estimating KL Divergence\n",
    "$$\n",
    "\\begin{align} \n",
    "KL(P_{\\theta ^*}, P_{{\\theta }}) &= \\mathbb {E}_{\\theta ^*}[\\ln (\\frac{p_{\\theta ^*}(X)}{p_{\\theta}(X)}) ]=\\sum _{x \\in E} p_{\\theta ^*} \\ln p_{\\theta ^*}(x) - \\sum _{x \\in E} p_{\\theta ^*} \\ln p_{\\theta }(x)\\\\\n",
    "&= \\mathbb {E}_{\\theta ^*}[\\ln p_{\\theta ^*}(X) ] - \\mathbb {E}_{\\theta ^*}[\\ln p_\\theta (X)]\n",
    "\\end{align}\n",
    "$$\n",
    "So the function $θ \\mapsto KL(P_{θ^∗},P_θ)$ is of the form: (since the first term dose not depend on $\\theta$.)\n",
    "$$ 'constant' - \\mathbb {E}_{\\theta ^*}[\\ln p_\\theta (X)]$$\n",
    "By the law of large numbers, $\\displaystyle \\frac{1}{n} \\sum _{i = 1}^ n \\ln (p_\\theta (X_ i)) \\to \\mathbb {E}_{\\theta ^*}[\\ln p_\\theta ]$ in probability\n",
    "$$\\hat{\\text {KL}}(P_{\\theta ^*}, P_\\theta ) := \\mathbb {E}_{\\theta ^*}[\\ln p_{\\theta ^*}(X) ] - \\displaystyle \\frac{1}{n} \\sum _{i = 1}^ n \\ln (p_\\theta (X_ i)).$$\n",
    "Therefore, as shown above, while we cannot find $\\theta$ that minimizes $KL(P_{\\theta ^*}, P_{{\\theta }})$, we can find $\\theta$ that minimizes $\\hat{\\text {KL}}(P_{\\theta ^*}, P_\\theta )$.\n",
    "\n",
    "## Maximum Likelihood principle\n",
    "$$\\hat{\\text {KL}}(P_{\\theta ^*}, P_\\theta ) := \\mathbb {E}_{\\theta ^*}[\\ln p_{\\theta ^*}(X) ] - \\displaystyle \\frac{1}{n} \\sum _{i = 1}^ n \\ln (p_\\theta (X_ i)).$$\n",
    "$$\n",
    "\\begin{align} \n",
    "\\min _{\\theta \\in \\Theta}\\hat{\\text {KL}}(P_{\\theta ^*}, P_\\theta ) &\\iff \\min _{\\theta \\in \\Theta} -\\displaystyle \\frac{1}{n} \\sum _{i = 1}^ n \\ln (p_\\theta (X_ i))\\\\\n",
    "&\\iff \\max _{\\theta \\in \\Theta } \\displaystyle \\frac{1}{n} \\sum _{i = 1}^ n \\ln (p_\\theta (X_ i))\\\\\n",
    "&\\iff \\max _{\\theta \\in \\Theta } \\displaystyle \\ln \\Bigg [\\prod \\limits_{i=1}^n p_\\theta (X_ i)\\Bigg]\\\\\n",
    "&\\iff \\max _{\\theta \\in \\Theta } \\displaystyle \\prod \\limits_{i=1}^n p_\\theta (X_ i)\\\\\n",
    "\\end{align}\n",
    "$$"
   ],
   "id": "f941a9e5cf280eca"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Likelihood of a Discrete Distribution",
   "id": "bc636760fc31ec03"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Likelihood of a Continuous Distribution",
   "id": "7bc0955b9117dc72"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
