{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Maximum Likelihood Estimator\n",
    "## Definition\n",
    "\n",
    "Let $X_1, \\ldots , X_ n \\stackrel{iid}{\\sim } \\mathbf{P}_{\\theta ^*}$ be discrete random variables. We construct a statistical model $(E, \\{ \\mathbf{P}_\\theta \\} _{\\theta \\in \\mathbb {R}})$ where $\\mathbf{P}_\\theta$ has pmf $p_\\theta$. We observe our sample to be $X_1 = x_1, X_2 = x_2, \\ldots , X_ n = x_ n$.   \n",
    "The maximum likelihood estimator for $\\theta ^*$ is defined to be$$\\hat{\\theta _ n}^{MLE} = \\text {argmax}_{\\theta \\in \\mathbb {R}}L_ n(X_1, \\ldots , X_ n, \\theta )$$\n",
    "$$\\text where \\quad\tL_ n(X_1, \\ldots , X_ n, \\theta ) = \\left( \\prod _{i = 1}^ n p_\\theta (X_ i) \\right).$$\n",
    "Remark (Log-likelihood estimator): In practice, we use the fact that$$\\theta_n^{MLE} = {argmax}_{\\theta \\in \\mathbb {R}}\\ln L_ n(X_1, \\ldots , X_ n, \\theta )$$\n",
    "\n",
    "## Interpretation of MLE\n",
    "* MLE is the value of $\\theta$ that maximizes the probability that $\\mathbf{P}_\\theta$ generates the data set $(x_1, \\ldots , x_ n)$.  \n",
    "  Since the likelihood is the joint density of $n$ iid samples from $\\mathbf{P}_\\theta$,$$\\mathbf{P}_\\theta [X_1 = x_1, \\ldots , X_ n = x_ n] = L_ n(x_1, \\ldots , x_ n, \\theta ).$$\n",
    "  Hence, the MLE finds $\\hat{\\theta }_ n$ that maximizes the probability that $x_1, \\ldots , x_ n$ were sampled from $P_{\\hat{\\theta }_ n}$.\n",
    "* MLE is the value of $\\theta$ that minimizes an estimator of the KL divergence between $\\mathbf{P}_\\theta$ and the true distribution $\\mathbf{P}_{\\theta ^*}$.  \n",
    "  In fact, this is how the MLE was derived from KL divergence.  \n",
    "\n",
    "**Remark**: Under some technical conditions the MLE is a **weakly consistent estimator** for $\\theta ^*$, meaning that the MLE will converge to $\\theta ^*$ in probability under these conditions. However, there are examples of statistical models where the maximum likelihood estimator will not converge to the true parameter.\n",
    "\n",
    "## Minimizing and Maximizing Functions\n",
    "If in addition $g$ is twice differentiable in the interval $I$, i.e. $g^{\\prime \\prime }(x)$ exists for all $x \\in I$, then $g$ is  \n",
    "concave if and only if $g^{\\prime \\prime }(x) {\\color{blue}{\\leq }}  0$ for all $x \\in I$;  \n",
    "strictly concave if $g^{\\prime \\prime }(x) {\\color{blue}{<}}  0$ for all $x \\in I$;  \n",
    "convex if and only if $g^{\\prime \\prime }(x) {\\color{blue}{\\geq }}  0$ for all $x \\in I$;  \n",
    "strictly convex if $g^{\\prime \\prime }(x) {\\color{blue}{>}}  0$ for all $x \\in I$;  \n",
    "\n",
    "Strictly concave functions are easy to maximize: if they have a maximum, then it is unique. It is the unique solution to$$h^â€²(\\theta)=0$$\n",
    "or, in the multivariate case$$\\nabla h(\\theta) = 0 \\in \\mathbb{R}^d$$\n",
    "There are many algorithms to find it numerically: this is the theory of \"convex optimization\". In this class, often a closed form formula for the maximum."
   ],
   "id": "6fbd8031fba253f1"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Examples ",
   "id": "fde83f97381de530"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
